# Reinforcement-Learning

## ¬øQu√© es RL?

El aprendizaje reforzado (Reinforcement Learning, RL) es un paradigma de la inteligencia artificial en el que un agente aprende a tomar decisiones a trav√©s de la interacci√≥n directa con su entorno, recibiendo recompensas o penalizaciones seg√∫n la calidad de sus acciones. En el √°mbito de las finanzas y, espec√≠ficamente, en el trading, esta metodolog√≠a resulta especialmente √∫til debido a la naturaleza din√°mica y estoc√°stica de los mercados. Mediante la recopilaci√≥n y el an√°lisis de informaci√≥n procedente de precios hist√≥ricos, indicadores t√©cnicos y datos macroecon√≥micos, el agente de RL puede ajustar continuamente su estrategia con el objetivo de maximizar la ganancia o el rendimiento ajustado al riesgo. Este enfoque contrasta con los m√©todos tradicionales, que suelen basarse en modelos est√°ticos y no son tan flexibles frente a cambios s√∫bitos en las condiciones de mercado.

## El Deep Q-Learning

Una de las t√©cnicas m√°s prominentes dentro del RL es el Deep Q-Learning (DQN), introducido por el equipo de DeepMind. El DQN fusiona el cl√°sico algoritmo de Q-Learning con redes neuronales profundas, lo que permite lidiar con espacios de estado de gran dimensi√≥n o continuos, comunes en el trading financiero. Al emplear estas redes para aproximar la funci√≥n de valor Q, el agente puede aprender a seleccionar acciones (comprar, vender o mantener) a partir de grandes vol√∫menes de datos de mercado, mejorando su capacidad de generalizaci√≥n.

## Aplicando el DQL para blackjack

'blackjack.py' entrena un agente de Deep Q-Learning (DQN) en el entorno de Blackjack de Gymnasium. El agente utiliza una red neuronal densa para predecir los valores Q (una salida por cada posible acci√≥n: ‚Äúhit‚Äù o ‚Äústand‚Äù) y toma decisiones seg√∫n una pol√≠tica epsilon-greedy, la cual va reduciendo el factor 
ùúñ
œµ a lo largo del entrenamiento para pasar gradualmente de la exploraci√≥n a la explotaci√≥n.

El c√≥digo gestiona un replay buffer (almacenamiento de transiciones) de donde se extraen muestras aleatorias para entrenar la red, rompiendo la correlaci√≥n temporal de los datos. Para mejorar la estabilidad del aprendizaje, se emplea una red objetivo que se sincroniza peri√≥dicamente con la red principal. Al final, se guarda la red entrenada en un archivo y se ofrece una funci√≥n para jugar un episodio usando la pol√≠tica aprendida.


## Resultados

El agente realiza su entrenamiento a lo largo de 2500 episodios. Cada 100 episodios, se registra en los logs la recompensa acumulada junto con el valor de epsilon.

![image](https://github.com/user-attachments/assets/a2240a6a-547e-4bfb-b9fc-b80e64bf4b87)

